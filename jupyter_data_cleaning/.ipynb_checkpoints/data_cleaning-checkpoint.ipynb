{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modules to import\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "import subprocess\n",
    "from mosestokenizer import MosesTokenizer, MosesDetokenizer\n",
    "# These are both modules that I need to create files for in the folder\n",
    "from apply_bpe import BPE\n",
    "import build_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download data\n",
    "We can usually skip this bit because we already have it downloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO add a check to the directory so we can uncomment this code and \n",
    "# make it into an if statement instead\n",
    "# dpath = \"./data\"\n",
    "# build_data.make_dir(dpath)\n",
    "# fname = 'traindev.zip'\n",
    "# url = 'http://www.macs.hw.ac.uk/InteractionLab/E2E/data/' + fname\n",
    "# build_data.download(url, dpath, fname)\n",
    "# build_data.untar(dpath, fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create files with byte pair encoding\n",
    "We're not going to use byte pair encoding anymore since the dataset is probably not diverse enough to warrant it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_source_target_files(input_file):\n",
    "    bpe_codes = open('../data/trainset.2000.bpe.codes')\n",
    "    encoder = BPE(bpe_codes)\n",
    "    word_tok = MosesTokenizer(no_escape=True)\n",
    "    def tokenize(text):\n",
    "        word_tokens = word_tok.tokenize(text)\n",
    "        sub_word_tokens = encoder.segment(' '.join(word_tokens))\n",
    "        return sub_word_tokens\n",
    "    outpath = os.path.dirname(input_file)\n",
    "    file_name = os.path.basename(os.path.splitext(input_file)[0])\n",
    "    target_file = open(os.path.join(outpath, file_name + \"-target.tok.2000.bpe\"), 'w')\n",
    "    source_file = open(os.path.join(outpath, file_name + \"-source.tok.2000.bpe\"), 'w')\n",
    "    input_csv = csv.reader(open(input_file, newline=''), delimiter=',', quotechar='\"')\n",
    "    # skip the first line in the csv with the column headers\n",
    "    next(input_csv)\n",
    "#     from IPython.core.debugger import Tracer; Tracer()() \n",
    "    for line in input_csv:\n",
    "        meaning_representations = line[0].split(', ')\n",
    "        acts_tok_bpe = []\n",
    "        for act in meaning_representations:\n",
    "            act_type = act[0:act.find(\"[\")].replace(' ', '')\n",
    "            acts_tok_bpe += ['__start_' + act_type + '__']\n",
    "            acts_tok_bpe += [tokenize(act[act.find(\"[\")+1:act.find(\"]\")])]\n",
    "            acts_tok_bpe += ['__end_' + act_type + '__']\n",
    "        acts = ' '.join(acts_tok_bpe).strip()\n",
    "        target = tokenize(line[1]).strip()\n",
    "        source_file.write(acts + '\\n')\n",
    "        target_file.write(target + '\\n')\n",
    "    source_file.close()\n",
    "    target_file.close()\n",
    "            \n",
    "def create_bpe_dict(input_file, num_operations):\n",
    "    # Inputs\n",
    "    # file name\n",
    "    # TODO BPE size\n",
    "    word_tok = MosesTokenizer(no_escape=True)\n",
    "    def tokenize(text):\n",
    "        word_tokens = word_tok.tokenize(text)\n",
    "        # I do feel like I should name the second\n",
    "        # variable something other than word_tokens\n",
    "        word_tokens = ' '.join(word_tokens)\n",
    "        return word_tokens\n",
    "    outpath = os.path.dirname(input_file)\n",
    "    file_name = os.path.basename(os.path.splitext(input_file)[0])\n",
    "    bpe_input_file = open(os.path.join(outpath, file_name + '-bpe-input.txt'), 'w')\n",
    "    input_csv = csv.reader(open(input_file, newline=''), delimiter=',', quotechar='\"')\n",
    "    # skip the first line in the csv with the column headers\n",
    "    next(input_csv)\n",
    "    for line in input_csv:\n",
    "        meaning_representations = line[0].split(', ')\n",
    "        acts_tokenized = []\n",
    "        for act in meaning_representations:\n",
    "            # https://stackoverflow.com/a/4894156/4507677 - instead of regex\n",
    "            acts_tokenized += [tokenize(act[act.find(\"[\")+1:act.find(\"]\")])]\n",
    "        acts = ' '.join(acts_tokenized)\n",
    "        target = tokenize(line[1])\n",
    "        bpe_input_file.write(acts + '\\n' + target + '\\n')\n",
    "    bpe_input_file.close()\n",
    "    # There were some errors with permissions for running this command so we're going \n",
    "    # to leave it and do it manually for now\n",
    "    # We fixed the permission issues and I might try to integrate this into the code later. We were missing\n",
    "    # execute rights on the python module\n",
    "    # ./learn_bpe.py -s {num_operations} < {train_file} > {codes_file}\n",
    "#     subprocess.run(['./learn_bpe.py', '-s', str(num_operations), '<', os.path.join(outpath, file_name + '-bpe-input.txt'),\n",
    "#                    '>', os.path.join(outpath, file_name + '.' + str(num_operations) + '.bpe.codes') ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think what we were trying to do here was be too fancy with the data cleaning and copy how they did it in parlai. Better to keep it simple. They were trying to make a generic thing that could handle lots of different ways of downloading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build(dpath):\n",
    "    # dpath = ./data\n",
    "    # http://www.macs.hw.ac.uk/InteractionLab/E2E/data/traindev.zip\n",
    "    if not build_data.built(dpath):\n",
    "        print('building data')\n",
    "        if build_data.built(dpath):\n",
    "            # An older version exists, so remove the outdated files\n",
    "            # TODO it would be better if this just removed the cleaning \n",
    "            # steps rather than forcing it to redownload the data again\n",
    "            build_data.remove_dir(dpath)\n",
    "        # Download the training and validation data\n",
    "        fname = 'traindev.zip'\n",
    "        url = 'http://www.macs.hw.ac.uk/InteractionLab/E2E/data/' + fname\n",
    "        build_data.download(url, dpath, fname)\n",
    "        build_data.untar(dpath, fname)\n",
    "\n",
    "        create_source_target_files(os.path.join(dpath, \"trainset.csv\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Clean and tokenize\n",
    "\n",
    "We have devset and trainset csvs. All we need is a step of steps that will output the same stuff we already are doing, except without the byte pair encoding. \n",
    "\n",
    "`TODO` Eventually we might consider trying to attach the categories to the tokens as features `token|category` rather than enclosed `__start__ token __end__`\n",
    "\n",
    "\n",
    "### Progress\n",
    "\n",
    "We added sentence tokenization to deal with trailing full stops. And decided to start doing aggresive hyphenation removal with moses tokenizer. Hence why we replace the token the moses tokenizer uses `'\\@-\\@'` with a regular hyphen.\n",
    "\n",
    "## TODO\n",
    "\n",
    "Add test data to the processing steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "import os\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/test_e2e.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-3282df5d84b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0msource_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"-source.tok\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0madditional_words_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"-additional-words-source.tok\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0minput_csv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquotechar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\"'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0;31m# skip the first line in the csv with the column headers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_csv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/test_e2e.csv'"
     ]
    }
   ],
   "source": [
    "valid_file_name = \"../data/devset.csv\"\n",
    "train_file_name = \"../data/trainset.csv\"\n",
    "test_file_name = \"../data/test_e2e.csv\"\n",
    "\n",
    "word_tok = MosesTokenizer('en')\n",
    "def tokenized(text):\n",
    "    #word_tokens = word_tok.tokenize(text, agressive_dash_splits=True, escape=False)\n",
    "    word_tokens = word_tok(text)\n",
    "    return word_tokens\n",
    "\n",
    "for input_file in [train_file_name, valid_file_name, test_file_name]:\n",
    "    outpath = os.path.dirname(input_file)\n",
    "    file_name = os.path.basename(os.path.splitext(input_file)[0])\n",
    "    target_file = open(os.path.join(outpath, file_name + \"-target.tok\"), 'w')\n",
    "    source_file = open(os.path.join(outpath, file_name + \"-source.tok\"), 'w')\n",
    "    additional_words_file = open(os.path.join(outpath, file_name + \"-additional-words-source.tok\"), 'w')\n",
    "    input_csv = csv.reader(open(input_file, newline=''), delimiter=',', quotechar='\"')\n",
    "    # skip the first line in the csv with the column headers\n",
    "    next(input_csv)\n",
    "#     from IPython.core.debugger import Tracer; Tracer()() \n",
    "    for line in input_csv:\n",
    "        meaning_representations = line[0].split(', ')\n",
    "        acts_tok = []\n",
    "        additional_words_tok = []\n",
    "        for act in meaning_representations:\n",
    "            act_type = act[0:act.find(\"[\")].replace(' ', '')\n",
    "            acts_tok += ['__start_' + act_type + '__']\n",
    "            acts_tok += word_tok(act[act.find(\"[\")+1:act.find(\"]\")])\n",
    "            acts_tok += ['__end_' + act_type + '__']\n",
    "            if act_type in ['near']:\n",
    "                additional_words_tok += ['near']\n",
    "            elif act_type not in ['name']:\n",
    "                additional_words_tok += ['_'.join(word_tok(act[act.find(\"[\")+1:act.find(\"]\")]))]\n",
    "        acts = ' '.join(acts_tok).strip().replace('\\@-\\@', '-')\n",
    "        source_file.write(acts + '\\n')\n",
    "        additional_words = ' '.join(additional_words_tok).strip().replace('\\@-\\@', '-')\n",
    "        additional_words_file.write(additional_words + '\\n')\n",
    "        if len(line) > 1:\n",
    "            target_tok = [word_tok(t) for t in sent_tokenize(line[1])]\n",
    "            target = ' '.join([j for i in target_tok for j in i]).strip().replace('\\@-\\@', '-')\n",
    "            target_file.write(target + '\\n')\n",
    "    source_file.close()\n",
    "    target_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize.toktok import ToktokTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = ToktokTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "tokenize() missing 1 required positional argument: 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-dd96ec738707>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: tokenize() missing 1 required positional argument: 'text'"
     ]
    }
   ],
   "source": [
    "a.tokenize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /soe/nivarghe/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
